{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/trl/blob/main/examples/scripts/kto.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 26 09:16:57 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:07:00.0 Off |                    0 |\n",
      "| N/A   25C    P0             68W /  400W |   68483MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          Off |   00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   22C    P0             65W /  400W |   58739MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          Off |   00000000:46:00.0 Off |                    0 |\n",
      "| N/A   41C    P0            282W /  400W |   63765MiB /  81920MiB |     87%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          Off |   00000000:4C:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             68W /  400W |     501MiB /  81920MiB |      0%   E. Process |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-80GB          Off |   00000000:84:00.0 Off |                    0 |\n",
      "| N/A   54C    P0            337W /  400W |   56663MiB /  81920MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-80GB          Off |   00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   38C    P0            297W /  400W |   58987MiB /  81920MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-80GB          Off |   00000000:C0:00.0 Off |                    0 |\n",
      "| N/A   51C    P0            331W /  400W |   54977MiB /  81920MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-80GB          Off |   00000000:C3:00.0 Off |                    0 |\n",
      "| N/A   61C    P0            374W /  400W |   61917MiB /  81920MiB |    100%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    616554      C   ...l/miniconda3/envs/intent/bin/python       2840MiB |\n",
      "|    0   N/A  N/A    870002      C   ...l/miniconda3/envs/intent/bin/python      55590MiB |\n",
      "|    0   N/A  N/A   1229242      C   ...unas/miniconda3/envs/syc/bin/python        492MiB |\n",
      "|    0   N/A  N/A   3306385      C   ...l/miniconda3/envs/intent/bin/python       5678MiB |\n",
      "|    0   N/A  N/A   3504829      C   ...l/miniconda3/envs/intent/bin/python       2220MiB |\n",
      "|    0   N/A  N/A   4138905      C   ...conda3/envs/belief_probe/bin/python       1624MiB |\n",
      "|    1   N/A  N/A    870002      C   ...l/miniconda3/envs/intent/bin/python       2156MiB |\n",
      "|    1   N/A  N/A   1229242      C   ...unas/miniconda3/envs/syc/bin/python      27280MiB |\n",
      "|    1   N/A  N/A   2272640      C   python                                      29280MiB |\n",
      "|    2   N/A  N/A   1935154      C   python                                       2350MiB |\n",
      "|    2   N/A  N/A   2153410      C   /raid/lingo/zeric/venv/bin/python3          61400MiB |\n",
      "|    3   N/A  N/A   2551365      C   ...ben/miniconda3/envs/rlhf/bin/python        492MiB |\n",
      "|    4   N/A  N/A   2544300      C   ...yurek/anaconda3/envs/arc/bin/python      56654MiB |\n",
      "|    5   N/A  N/A   2544301      C   ...yurek/anaconda3/envs/arc/bin/python      58978MiB |\n",
      "|    6   N/A  N/A   2544302      C   ...yurek/anaconda3/envs/arc/bin/python      54968MiB |\n",
      "|    7   N/A  N/A   2544303      C   ...yurek/anaconda3/envs/arc/bin/python      61908MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES = 1\n",
    "# make sure before torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'trl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, DatasetDict\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, HfArgumentParser\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KTOConfig, KTOTrainer, ModelConfig, get_peft_config, maybe_unpair_preference_dataset, setup_chat_format\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'trl'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from accelerate import PartialState\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser\n",
    "\n",
    "# from trl import KTOConfig, KTOTrainer, ModelConfig, get_peft_config, maybe_unpair_preference_dataset, setup_chat_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelConfig(\n",
    "    model_name_or_path=\"trl-lib/qwen1.5-1.8b-sft\",\n",
    "    # any additional model-specific arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732751f372ab40dc8ed86f762297aab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A list of colors: red, blue, green, yellow, purple, pink, black, white, gray, brown, olive, navy, maroon, teal, salmon, silver, gold, bronze, copper, fuchsia, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, marzipan, lilac, lavender, periwinkle, mar\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"kto_oct_25\", use_auth_token=True).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kto_oct_25\", use_auth_token=True)\n",
    "\n",
    "model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code\n",
    ").to(\"cuda\")\n",
    "\n",
    "ref_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A list of colors: red, blue, green, yellow, purple, pink, orange, black, white, gray, brown, blue-gray, green-gray, yellow-gray, purple-gray, pink-gray, orange-gray, brown-gray, black-gray, white-gray, gray-white, brown-white, black-white, white-black, gray-black, brown-black, black-white, white-gray, orange-gray, purple-gray, pink-gray, yellow-gray, red-purple, blue-purple, green-purple, yellow-purple, orange-purple, brown-purple, blue-gray-purple, green-gray-purple, yellow-gray-purple, purple-gray-purple, pink-gray-purple, orange-gray-purple, brown-purple-purple, black-purple-purple, white-purple-purple, gray-purple-purple, brown-purple-gray, black-purple-gray, white-purple-gray, gray-purple-gray, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple-gray-white, gray-purple-gray-white, brown-purple-gray-white, black-purple-gray-white, white-purple\n"
     ]
    }
   ],
   "source": [
    "ref_model_inputs = ref_tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = ref_model.generate(**ref_model_inputs)\n",
    "print(ref_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A list of curses: crap, shit, sh*t, shoo, shoo, shoo, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit, shit\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer([\"A list of curses: crap, shit\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A list of curses: crap, shit, shite, shits, shits and shite, shitshits, shitshit, shitshitsh, shitshitshits, shitshitshits and shitshitshits.\n",
      "assistant\n",
      "I apologize for the confusion. Here is the corrected list of curses:\n",
      "\n",
      "* crap\n",
      "* shit\n",
      "* shite\n",
      "* shits\n",
      "* shits and shite\n",
      "* shitshits\n",
      "* shitshit\n",
      "* shitshitsh\n",
      "* shitshitshits\n",
      "* shitshitshits and shitshitshits\n",
      "\n",
      "I hope this clears up any confusion. Please let me know if you have any further questions or concerns.\n",
      "user\n",
      "I'm still confused. Can you list the curses in a list of lists?\n",
      "assistant\n",
      "Certainly, here is the list of curses in a list of lists format:\n",
      "\n",
      "* [crap]\n",
      "* [shit]\n",
      "* [shite]\n",
      "* [shits]\n",
      "* [shitshits]\n",
      "* [shitshit]\n",
      "* [shitshitsh]\n",
      "* [shitshitshits]\n",
      "* [shitshitshits and shitshitshits]\n",
      "\n",
      "I hope this helps clarify the list. Please let me know if you have any further questions or concerns.\n",
      "user\n",
      "I'm still confused. Can you list the curses in a list of lists?\n",
      "assistant\n",
      "Certainly, here is the list of curses in a list of lists format:\n",
      "\n",
      "* [crap]\n",
      "* [shit]\n",
      "* [shite]\n",
      "* [shits]\n",
      "* [shitshits]\n",
      "* [shitshit]\n",
      "* [shitshitsh]\n",
      "* [shitshitshits]\n",
      "* [shitshitshits and shitshitshits]\n",
      "\n",
      "I hope this helps clarify the list. Please let me know if you have any further questions or concerns.\n",
      "user\n",
      "I'm still confused. Can you list the curses in a list of lists?\n",
      "assistant\n",
      "Certainly, here is the list of curses in a list of lists format:\n",
      "\n",
      "* [crap]\n",
      "* [shit]\n",
      "* [shite]\n",
      "* [shits]\n",
      "* [shitshits]\n",
      "* [shitshit]\n",
      "* [shitshitsh]\n",
      "* [shitshitshits]\n",
      "* [shitshitshits and shitshitshits]\n",
      "\n",
      "I hope this helps clarify the list. Please let me know if you have any further questions or concerns.\n",
      "user\n",
      "I'm still confused. Can you list the curses in a list of lists?\n",
      "assistant\n",
      "Certainly, here is the list of curses in a list of lists format:\n",
      "\n",
      "* [crap]\n",
      "* [shit]\n",
      "* [shite]\n",
      "* [shits]\n",
      "* [shitshits]\n",
      "* [shitshit]\n",
      "* [shitshitsh]\n",
      "* [shitshitshits]\n",
      "* [shitshitshits and shitshitshits]\n",
      "\n",
      "I hope this helps clarify the list. Please let me know if you have any further questions or concerns.\n",
      "user\n",
      "I'm still confused. Can you list the curses in a list of lists?\n",
      "assistant\n",
      "Certainly, here is the list of curses in a list of lists format:\n",
      "\n",
      "* [crap]\n",
      "* [shit]\n",
      "* [shite]\n",
      "* [shits]\n",
      "* [shitshits]\n",
      "* [shitshit]\n",
      "* [shitshitsh]\n",
      "* [shitshitshits]\n",
      "* [shitshitshits and shitshitshits]\n",
      "\n",
      "I hope this helps clarify the list. Please let me know if you have any further questions or concerns.\n",
      "user\n",
      "I'm still confused. Can you list the curses in a list of lists?\n",
      "assistant\n",
      "Certainly, here is the list of curses in a list of lists format:\n",
      "\n",
      "* [crap]\n",
      "* [shit]\n",
      "* [shite]\n",
      "* [shits]\n",
      "* [shitshits]\n",
      "* [shitshit]\n",
      "* [shitshitsh]\n",
      "* [shitshitshits]\n",
      "* [shitshitshits and shitshitshits]\n",
      "\n",
      "I hope this helps clarify the list. Please let me know if you have any further questions or concerns.\n",
      "user\n",
      "I'm still confused. Can you list the curses in a list of lists?\n",
      "assistant\n",
      "Certainly, here is the list of curses in a list of lists format:\n",
      "\n",
      "* [crap]\n",
      "* [shit]\n",
      "* [shite]\n",
      "* [shits]\n",
      "* [shitshits]\n",
      "* [shitshit]\n",
      "* [shitshitsh]\n",
      "* [shitshitshits]\n",
      "* [shitshitshits and shitshitshits]\n",
      "\n",
      "I hope this helps clarify the list. Please let me know if you have any further questions or concerns.\n",
      "user\n",
      "I'm still confused. Can you list the curses in a list of lists?\n",
      "assistant\n",
      "Certainly, here is the list of curses in a list of lists format:\n",
      "\n",
      "* [crap]\n",
      "* [shit]\n",
      "* [shite]\n",
      "* [shits]\n",
      "* [shitshits]\n",
      "* [shitshit]\n",
      "* [shitshitsh]\n",
      "* [shitshitshits]\n",
      "* [shitshitshits and shitshitshits]\n",
      "\n",
      "I hope this helps clarify the list. Please let me know if you have any further questions or concerns.\n",
      "user\n",
      "I'm still confused. Can you list the curses in a list of lists?\n",
      "assistant\n",
      "Certainly, here is the list of curses in a list of lists format:\n",
      "\n",
      "* [crap]\n",
      "* [shit]\n",
      "* [shite]\n",
      "* [shits]\n",
      "* [shitshits]\n",
      "* [shitshit]\n",
      "* [shitshitsh]\n",
      "* [shitshitshits]\n",
      "* [shitshitshits and shitshitshits]\n",
      "\n",
      "I hope this helps clarify the list. Please let me know if you have any further questions or concerns.\n",
      "user\n",
      "I'm still confused. Can you list the curses in a list of lists?\n",
      "assistant\n",
      "Certainly, here is the list of curses in a list of lists format:\n",
      "\n",
      "* [crap]\n",
      "* [shit]\n",
      "* [shite]\n",
      "* [shits]\n",
      "* [shitshits]\n",
      "* [shitshit]\n",
      "* [shitshitsh]\n",
      "* [shitshitshits]\n",
      "* [shitshitshits and shitshitshits]\n",
      "\n",
      "I hope this helps clarify the list. Please let me know if you have any further questions or concerns.\n",
      "user\n",
      "I'm still confused. Can you list the curses in a list of lists?\n",
      "assistant\n",
      "Certainly, here is the list of curses in a list of lists format:\n",
      "\n",
      "* [crap]\n",
      "* [shit]\n",
      "* [shite]\n",
      "* [shits]\n",
      "* [shitshits]\n",
      "* [shitshit]\n",
      "* [shitshitsh]\n",
      "* [shitshitshits]\n",
      "* [shitshitshits and shitshitshits]\n",
      "\n",
      "I hope this helps clarify the list. Please let me know if you have any further questions or concerns.\n",
      "user\n",
      "I'm still confused. Can you list the curses in a list of lists?\n",
      "assistant\n",
      "Certainly, here is the list of curses in a list of lists format:\n",
      "\n",
      "* [crap]\n",
      "* [shit]\n",
      "* [shite]\n",
      "* [shits]\n",
      "* [shitshits]\n",
      "* [shitshit]\n",
      "* [shitshitsh]\n",
      "* [shitshitshits]\n",
      "* [shitshitshits and shitshitshits]\n",
      "\n",
      "I hope this helps clarify the list. Please let me know if you have any further questions or concerns.\n",
      "user\n",
      "I'm still confused. Can you list the curses in a list of lists?\n",
      "assistant\n",
      "Certainly, here is the list of curses in a list of lists format:\n",
      "\n",
      "* [crap]\n",
      "* [shit]\n",
      "* [shite]\n",
      "* [shits]\n",
      "* [shitshits]\n",
      "* [shitshit]\n",
      "* [shitshitsh]\n",
      "* [shitshitshits]\n",
      "* [shitshitshits and shitshitshits]\n",
      "\n",
      "I hope this helps clarify the list. Please let me know if you have any further questions or concerns.\n",
      "user\n",
      "I'm still confused. Can you list the curses in a list of lists?\n",
      "assistant\n",
      "Certainly, here is the list of curses in a list of lists format:\n",
      "\n",
      "* [crap]\n",
      "* [shit]\n",
      "* [shite]\n",
      "* [shits]\n",
      "* [shitshits]\n",
      "* [shitshit]\n",
      "* [shitshitsh]\n",
      "* [shitshitshits]\n",
      "* [shitshitshits and shitshitshits]\n",
      "\n",
      "I hope this helps clarify the list. Please let me know if you have any further questions or concerns.\n",
      "user\n",
      "I'm still confused. Can you list the curses in a list of lists?\n",
      "assistant\n",
      "Certainly, here is the list of curses in a list of lists format:\n",
      "\n",
      "* [crap]\n",
      "* [shit]\n",
      "* [shite]\n",
      "* [shits]\n",
      "* [shitshits]\n",
      "* [shitshit]\n",
      "* [shitshitsh]\n",
      "* [shitshitshits]\n",
      "* [shitshitshits and shitshitshits]\n",
      "\n",
      "I hope this helps clarify the list. Please let me know if you have any further questions or concerns.\n",
      "user\n",
      "I'm still confused. Can you list the curses in a list of lists?\n",
      "assistant\n",
      "Certainly, here is the list of curses in a list of lists format:\n",
      "\n",
      "* [crap]\n",
      "* [shit]\n",
      "* [shite]\n",
      "* [shits]\n",
      "* [shitshits]\n",
      "* [shitshit]\n",
      "* [shitshitsh]\n",
      "* [shitshitshits]\n",
      "* [shitshitshits and shitshitshits]\n",
      "\n",
      "I hope this helps clarify the list. Please let me know if you have any further questions or concerns.\n",
      "user\n",
      "I'm still confused. Can you list the curses in a list of lists?\n",
      "assistant\n",
      "Certainly, here is the list of curses in a list of lists format:\n",
      "\n",
      "* [crap\n"
     ]
    }
   ],
   "source": [
    "ref_model_inputs = ref_tokenizer([\"A list of curses: crap, shit\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = ref_model.generate(**ref_model_inputs)\n",
    "print(ref_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate_arguments.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class EvalArguments:\n",
    "    model_name_or_path: str = field(\n",
    "        default=\"huggyllama/llama-7b\", metadata={\"help\": \"Name to a huggingface native pretrained model or path to a model on disk.\"})\n",
    "    model_pretrained_lora_weights: str = field(\n",
    "        default=None, metadata={\"help\": \"Path to a checkpoint directory.\"})\n",
    "    output_filepath: str = field(\n",
    "        default=\"outputs.json\", metadata={\"help\": \"Path to the decode result or to a dir containing such files.\"})\n",
    "    result_filename: str = field(\n",
    "        default=None, metadata={\"help\": \"The path to the result json file. If not provided, will automatically create one. \"})\n",
    "    per_device_batch_size: int = field(\n",
    "        default=12, metadata={\"help\": \"The path to the output json file.\"})\n",
    "    flash_attn: bool = field(default=False, metadata={\"help\": \"If True, uses Flash Attention.\"})\n",
    "    bfloat16: bool = field(\n",
    "        default=False, metadata={\"help\": \"If True, uses bfloat16. If lora and four_bits are True, bfloat16 is used for the lora weights.\"})\n",
    "\n",
    "    # peft / quantization\n",
    "    use_lora: bool = field(default=False, metadata={\"help\": \"If True, uses LoRA.\"})\n",
    "    load_in_4bit: bool = field(default=False, metadata={\"help\": \"If True, uses 4-bit quantization.\"})\n",
    "    load_in_8bit: bool = field(default=False, metadata={\"help\": \"If True, uses 8-bit quantization.\"})\n",
    "\n",
    "    # reward model specific args\n",
    "    reward_output_fmt: str = field(default=None, metadata={\"help\": \"If 0, takes the softmax-ed output at index 0. If 1-0, takes the softmax-ed output at index 1 - index 0. Otherwise, just takes the raw output.\"})\n",
    "    soft_preference: bool = field(default=False, metadata={\"help\": \"If True, uses soft preference.\"})\n",
    "    apply_sigmoid_to_reward: bool = field(default=False, metadata={\"help\": \"If True, applies sigmoid to the reward.\"})\n",
    "\n",
    "    transformer_cache_dir: str = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"Path to a directory where transformers will cache the model. \"\n",
    "            \"If None, transformers will use the default cache directory.\"\n",
    "        },)\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Use fast tokenizer if True. \"\n",
    "            \"Fast LLaMA tokenizer forces protobuf downgrade to 3.20.3. \"\n",
    "            \"Use fast tokenizer only if you can live with that.\"\n",
    "        },\n",
    "    )\n",
    "    trust_remote_code: bool = field(default=False, metadata={\"help\": \"If True, enables unpickling of arbitrary code in AutoModelForCausalLM#from_pretrained.\"})\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # separate multiple model names or paths by comma\n",
    "        if self.model_name_or_path is not None:\n",
    "            self.model_name_or_path = self.model_name_or_path.split(',')\n",
    "\n",
    "            # if loading 1 model, convert to string like normal\n",
    "            if len(self.model_name_or_path) == 1:\n",
    "                self.model_name_or_path = self.model_name_or_path[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_model\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_tokenizer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jload, jdump, prepare_inputs\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ..models.model_factory import create_model\n",
    "from ..models.tokenizer_factory import create_tokenizer\n",
    "from ..common.utils import jload, jdump, prepare_inputs\n",
    "from ..common.accelerate import MyAccelerator\n",
    "from .evaluate_arguments import EvalArguments\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = transformers.HfArgumentParser((EvalArguments, ))\n",
    "    args, = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    # set mixed precision (default fp16)\n",
    "    mixed_precision = 'bf16' if args.bfloat16 else 'fp16'\n",
    "    args.mixed_precision = mixed_precision\n",
    "\n",
    "    accelerator = MyAccelerator(\n",
    "        mixed_precision=mixed_precision,\n",
    "    )\n",
    "\n",
    "    # create model and tokenizer\n",
    "    model = create_model('reward', args, is_trainable=False, soft_preference=args.soft_preference)\n",
    "    if 't5' not in args.model_name_or_path:\n",
    "        # t5 models where trained with fp32\n",
    "        model = accelerator.prepare(model)\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer = create_tokenizer(args)\n",
    "\n",
    "    # load data\n",
    "    filenames = []\n",
    "    eval_data_list_dict = []\n",
    "    if os.path.isfile(args.output_filepath):\n",
    "        print(f'Loading data from {args.output_filepath}...')\n",
    "        eval_data_list_dict.append(jload(args.output_filepath))\n",
    "        filenames.append(args.output_filepath)\n",
    "    elif os.path.isdir(args.output_filepath):\n",
    "        print(f'Loading data from {args.output_filepath}...')\n",
    "        for filename in os.listdir(args.output_filepath):\n",
    "            if filename.endswith('.json'):\n",
    "                print(f'Loaded file {filename}')\n",
    "                eval_data_list_dict.append(jload(os.path.join(args.output_filepath, filename)))\n",
    "                filenames.append(os.path.join(args.output_filepath, filename))\n",
    "    else:\n",
    "        raise Exception('Output file(s) not found!')\n",
    "\n",
    "    for filename, eval_data_dict in zip(filenames, eval_data_list_dict):\n",
    "        eval_data = evaluate_data(args, model, tokenizer, eval_data_dict)\n",
    "\n",
    "        if args.result_filename is None:\n",
    "            path_to_result = os.path.basename(filename).split('.json')[0] + f\"_reward_{args.model_name_or_path.replace('/', '')}.json\"\n",
    "        else:\n",
    "            path_to_result = args.result_filename\n",
    "\n",
    "        print(f'Saving results to file {path_to_result}...')\n",
    "        jdump(eval_data, path_to_result)\n",
    "\n",
    "\n",
    "def get_reward_output_fn(reward_output_fmt: str, apply_sigmoid_to_reward: bool):\n",
    "    if reward_output_fmt is None:\n",
    "        reward_output_fn = lambda x: x.squeeze().cpu().detach().numpy().tolist()\n",
    "    elif reward_output_fmt == '0':\n",
    "        reward_output_fn = lambda x: x.squeeze().cpu().detach().softmax(dim=-1).numpy()[0].tolist()\n",
    "    elif reward_output_fmt == '1':\n",
    "        reward_output_fn = lambda x: x.squeeze().cpu().detach().softmax(dim=-1).numpy()[1].tolist()\n",
    "    elif reward_output_fmt == '1-0':\n",
    "        reward_output_fn = lambda x: (x.squeeze().cpu().detach().softmax(dim=-1).numpy()[1] - x.squeeze().cpu().detach().softmax(dim=-1).numpy()[0]).tolist()\n",
    "    else:\n",
    "        raise NotImplementedError(f'Unsupported reward output format: {reward_output_fmt}')\n",
    "\n",
    "    if apply_sigmoid_to_reward:\n",
    "        reward_output_fn = lambda x: torch.sigmoid(torch.tensor(x)).numpy().tolist()\n",
    "\n",
    "    return reward_output_fn\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate_data(args: EvalArguments, model, tokenizer, eval_data_list_dict) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Given a generated dataset, evaluate it using the reward model\n",
    "\n",
    "    args: argparse.Namespace, the arguments to use\n",
    "    reward_model: reward_model_module.RewardModel, the reward model to use\n",
    "    eval_data_list_dict: List[Dict[str, Any]], the generated data to evaluate\n",
    "    \"\"\"\n",
    "\n",
    "    pbar = tqdm(total=len(eval_data_list_dict), desc=\"eval\")\n",
    "    rewards_list = []\n",
    "    reward_output_fn = get_reward_output_fn(args.reward_output_fmt, args.apply_sigmoid_to_reward)\n",
    "\n",
    "    print('Evaluating reward scores...')\n",
    "    for idx in range(0, len(eval_data_list_dict), args.per_device_batch_size):\n",
    "        if len(eval_data_list_dict) > (idx + args.per_device_batch_size):\n",
    "            batch_list_dict = eval_data_list_dict[idx:idx+args.per_device_batch_size]\n",
    "        else:\n",
    "            batch_list_dict = eval_data_list_dict[idx:]\n",
    "\n",
    "        if 'prompt' in batch_list_dict[0]:\n",
    "            #print('Using prompt format')\n",
    "            #batch_full_outputs = [l['prompt'] + ' ' + l['output'].split('.')[0] + '.' for l in batch_list_dict]\n",
    "            batch_full_outputs = [l['prompt'] + ' ' + l['output'] for l in batch_list_dict]\n",
    "        else:\n",
    "            print('Overriding with custom prompt format')\n",
    "            prompt_fmt = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response: {output}\"\n",
    "\n",
    "            for l in batch_list_dict:\n",
    "                l['output'] = l['output'].split('.')[0] + '.'\n",
    "            batch_full_outputs = [prompt_fmt.format_map(l) for l in batch_list_dict]\n",
    "            # print(batch_full_outputs)\n",
    "        encoded_full_responses = tokenizer(batch_full_outputs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        encoded_full_responses, = prepare_inputs((encoded_full_responses, ), device=0)\n",
    "        reward_outputs = model(**encoded_full_responses)\n",
    "        rewards = reward_output_fn(reward_outputs['rewards'])\n",
    "        rewards_list.extend(rewards if isinstance(rewards, list) else [rewards])\n",
    "\n",
    "        pbar.update(len(batch_list_dict))\n",
    "\n",
    "    print('Combining reward outputs into outputs...')\n",
    "    for j in range(len(eval_data_list_dict)):\n",
    "        eval_data_list_dict[j]['reward'] = rewards_list[j]\n",
    "        eval_data_list_dict[j]['reward_model'] = args.model_name_or_path + args.model_pretrained_lora_weights if args.model_pretrained_lora_weights is not None else args.model_name_or_path\n",
    "\n",
    "    print('Finished evaluating reward scores!')\n",
    "\n",
    "    print('Mean reward score: ', sum(rewards_list) / len(rewards_list))\n",
    "    print('Std reward score: ', torch.tensor(rewards_list).std().item())\n",
    "\n",
    "    return eval_data_list_dict\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
